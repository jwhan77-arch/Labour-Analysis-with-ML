{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2db31a69-2eba-4187-8dd4-d0e3f4e83707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "#폴더에서 csv 전체를 읽고, 기준 데이터에서 피처를 선정\n",
    "\n",
    "def read_csv_and_select_feature(folder_path, drop_col, str_col, del_if_zero):\n",
    "    \n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    if not csv_files:\n",
    "        print (\"csv 파일을 찾을 수 없습니다.\")\n",
    "    else:\n",
    "        for file in csv_files:\n",
    "            print(f\"{os.path.basename(file)}\")\n",
    "        print(f\"총{len(csv_files)}개의 파일을 찾았습니다\\n\")\n",
    "\n",
    "    # 파일명을 통해 가장 최근 데이터를 선정하고 그 파일의 칼럼을 통합할 데이터의 기준으로\n",
    "    most_recent_file = None\n",
    "    max_year=0\n",
    "    year_pattern = re.compile(r'\\d{4}')\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        match = year_pattern.search(file_name)\n",
    "        if match:\n",
    "            current_year= int(match.group())\n",
    "            if current_year > max_year:\n",
    "                max_year = current_year\n",
    "                most_recent_file= file\n",
    "\n",
    "    try: \n",
    "        benchmark_col = pd.read_csv(most_recent_file, encoding=\"cp949\", nrows=0)\n",
    "    except UnicodeDecodeError:\n",
    "        benchmark_col = pd.read_csv(most_recent_file, encoding=\"uft-8\", nrows=0)\n",
    "    print (f\"가장 최근 조사인 {max_year}년 데이터에 {len(benchmark_col.columns)}개의 칼럼이 있습니다.\\n\")\n",
    "    \n",
    "    # drop_col에 있는 제외할 칼럼을 제거. \n",
    "    all_col = benchmark_col.columns.tolist()\n",
    "    filtered_col = [col for col in all_col if not any(re.search(keyword, col) for keyword in drop_col)]\n",
    "    print(f\"제외할 칼럼은 {drop_col}. {len(all_col)-len(filtered_col)}개의 칼럼이 제거되었습니다.\\n{len(filtered_col)}개의 칼럼을 조사합니다.\\n\")\n",
    "\n",
    "    #선택된 피쳐(열)가 있는 칼럼을 파일에서 읽은 후 데이타를 병합.\n",
    "    df_list=[]\n",
    "    warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "    \n",
    "    for file in csv_files:\n",
    "        \n",
    "        try: \n",
    "            df_col=pd.read_csv(file, encoding=\"cp949\", nrows=0).columns\n",
    "            csv_encoding=\"cp949\"\n",
    "        except UnicodeDecodeError:\n",
    "            df_col=pd.read_csv(file, encoding=\"utf-8\", nrows=0).columns\n",
    "            csv_encoding=\"uft-8\"\n",
    "    \n",
    "        col_to_load= list(set(filtered_col) & set(df_col))\n",
    "        if col_to_load != filtered_col:\n",
    "            print(f\"{os.path.basename(file)}에 {list(set(filtered_col) - set(df_col))}의 데이타가 없습니다.NaN으로 채웁니다.\")\n",
    "        df= pd.read_csv(file, encoding=csv_encoding, usecols=col_to_load)\n",
    "        df=df.reindex(columns=filtered_col)\n",
    "        \n",
    "        df=df[1:]\n",
    "        df_list.append(df)\n",
    "    \n",
    "    merged_df=pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\n총 데이터는 {len(merged_df.columns)}개의 열, {len(merged_df)}개의 행입니다.\\n\")\n",
    "\n",
    "    #str_col은 문자 코드. 숫자 코드로 변환\n",
    "    print(f\"{str_col}은 숫자 코드로 변환합니다.\\n\")\n",
    "    all_col=merged_df.columns.tolist()\n",
    "    numberic_col = list(set(all_col) - set(str_col))\n",
    "    for col in numberic_col:\n",
    "        merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "    le=LabelEncoder()\n",
    "    for col in str_col:\n",
    "        merged_df[col]=le.fit_transform(merged_df[col])\n",
    "    \n",
    "     #모델에 중요한 영향을 미치기 때문에 0이 되면 안 되는 피쳐 del_if_zero에서 값이 0인 샘플을 삭제\n",
    "    print(f\"{del_if_zero}에서 0이 될 수 없는 샘플을 삭제합니다.\\n\")\n",
    "    len_before=len(merged_df)\n",
    "    for col in del_if_zero:\n",
    "        merged_df=merged_df[merged_df[col]>0]\n",
    "    len_after =len(merged_df)\n",
    "    print(f\"{len_before - len_after}개의 샘플을 제거했습니다.\\n\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "#_____________________________________________________________________\n",
    "#0을 평균으로 대체, 0이 과반 이상이면 삭제\n",
    "\n",
    "def zero_processing(df):\n",
    "        \n",
    "    len_before=len(df.columns)\n",
    "    df_temp=df.fillna(0)\n",
    "    df_temp=df_temp.replace(0, np.nan)\n",
    "    df_mean=df_temp.mean().round()\n",
    "    df_final=df_temp.fillna(df_mean).dropna(axis=1)\n",
    "    len_after=len(df_final.columns)\n",
    "    print(f\"0을 평균값으로 대체. 열의 값이 모두 NaN인 경우 삭제.\\n {len_before - len_after}개 칼럼 삭제\")\n",
    "    \n",
    "    \n",
    "    print(f\"최종 {len(df_final.columns)-1}개의 피쳐를 조사합니다.\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "#_____________________________________________________________________\n",
    "#입사연월과 조사연월을 이용해 근속 계산 \n",
    "def len_of_serv_calculator(df, enterance_date_col_name,census_date_col_name):\n",
    "\n",
    "    enterance_date_col=[]\n",
    "    census_date_col=[]\n",
    "    len_of_service=[]\n",
    "    enterance_date_col = pd.to_datetime(df[enterance_date_col_name].astype(str), format='%Y%m')\n",
    "    census_date_col = pd.to_datetime(df[census_date_col_name].astype(str), format=\"%Y%m\")\n",
    "    \n",
    "    len_of_service=(census_date_col.dt.year - enterance_date_col.dt.year)*12 +(census_date_col.dt.month - enterance_date_col.dt.month)\n",
    "    df_len_of_service = pd.Series(len_of_service, name=\"재직월수\")\n",
    "    \n",
    "    df=pd.concat([df, df_len_of_service], axis=1)\n",
    "    df=df.drop(columns=[enterance_date_col_name,census_date_col_name])\n",
    "    print (f\"{enterance_date_col_name}과 {census_date_col_name}을 이용해 [재직월수] 칼럼을 만들었습니다.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#_____________________________________________________________________\n",
    "#XGboost 머신러닝\n",
    "\n",
    "def XGboost_ML(df, target_col, weight_col, n_estimators=30):\n",
    "    \n",
    "    X = df.drop([target_col, weight_col], axis=1)\n",
    "    y= df[target_col]\n",
    "    w= df[weight_col]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(X, y, w, test_size=0.3, random_state=42)\n",
    "    \n",
    "    model= xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=n_estimators, random_state=42)\n",
    "    model.fit(X_train, y_train, sample_weight=w_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred, sample_weight=w_test))\n",
    "    mean = y_test.mean()\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nXGBoost 모델 성능 (RMSE): {rmse:.2f} \\n 타깃 평균은 {mean:.2f}.\\n 평균 대비 RMSE는 {rmse/mean:.2%}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\\nXGBoost 모델의 SHAP 플롯을 생성합니다.\\n\")\n",
    "    X_train_10= X_train.sample(frac=0.1, random_state=1)\n",
    "    explainer = shap.Explainer(model, X_train)\n",
    "    s_values= explainer(X_train_10)\n",
    "    shap.plots.bar(s_values, max_display=10)\n",
    "   \n",
    "    print(\"--- XGBoost 모델 분석 완료 ---\")\n",
    "    return s_values\n",
    "\n",
    "def basic_analysis(df,target_col, weight_col):\n",
    "\n",
    "    import seaborn as sns\n",
    "    from statsmodels.stats.weightstats import DescrStatsW\n",
    "\n",
    "    plt.rc('font', family='NanumGothic')\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "    #상관계수 계산\n",
    "    weights =df[weight_col]\n",
    "    df_data=df.drop(columns=weight_col)\n",
    "    weighted_stats = DescrStatsW(df_data, weights=weights)\n",
    "    weighted_corr_matrix = weighted_stats.corrcoef\n",
    "    corr_df = pd.DataFrame(weighted_corr_matrix, columns=df_data.columns, index=df_data.columns)\n",
    "    corr_df_s =corr_df.stack().reset_index(name='correlation')\n",
    "   \n",
    "    #히트맵 \n",
    "    g=sns.relplot(data=corr_df_s, x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\", palette=\"vlag\",hue_norm=(-1, 1), edgecolor=\".7\",\n",
    "        height=20, sizes=(50, 250), size_norm=(-.2, .8),)\n",
    "    g.set_xticklabels(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    #Target_col 히스토그램\n",
    "    threshold= df[target_col].quantile(0.99)\n",
    "    sns.displot(data=df, x=target_col, weights=weight_col, kind=\"kde\")\n",
    "    plt.xlim(0,threshold)\n",
    "    plt.show()\n",
    "\n",
    "    #상관계수 높은 주요 변수 그래프\n",
    "    top5_col= corr_df[target_col].abs().nlargest(6,keep='last').index\n",
    "    classification_col=[\"코드\",\"여부\"]\n",
    "    \n",
    "    for col in top5_col:\n",
    "     if col != target_col:\n",
    "         if any(keyword in col for keyword in classification_col):\n",
    "             sns.displot(data=df, x=target_col, weights=weight_col, hue=col, kind='kde')\n",
    "             plt.xlim(0,threshold)\n",
    "             plt.show()\n",
    "         else:\n",
    "             df_g= df[df[target_col]<threshold]\n",
    "             df_g=df_g.sample(frac=0.1)\n",
    "             sns.lmplot(data=df_g, x=target_col, y=col, scatter_kws={'s': 3, 'alpha': 0.1, 'color': 'orange'})\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # 중요 파라미터들\n",
    "    folder_path=\"data\" #csv 파일이 있는 폴더\n",
    "    drop_col = [\"_LABEL\"] #변수 설명하는 칼럼들\n",
    "    str_col = ['현재일관련사항_10차산업대분류코드', '이전직장_1자리_10차산업대분류코드'] #분류코드가 String인 칼럼\n",
    "    del_if_zero =[\"현재일관련사항_직장시작연월\", \"최근3개월간평균급여\" ] #값이 0인 경우 삭제해야 하는 샘플\n",
    "    enterance_date_col=\"현재일관련사항_직장시작연월\"\n",
    "    census_date_col =\"조사연월\"\n",
    "    weight_col = \"가중값\"\n",
    "    target_col=\"최근3개월간평균급여\"\n",
    "    \n",
    "    #함수 호출\n",
    "    df= read_csv_and_select_feature(folder_path=folder_path, drop_col=drop_col, str_col=str_col, del_if_zero=del_if_zero)\n",
    "    df= zero_processing(df=df)\n",
    "    df= len_of_serv_calculator(df=df, enterance_date_col_name=enterance_date_col, census_date_col_name=census_date_col )\n",
    "    basic_analysis(df=df, target_col=target_col, weight_col=weight_col)\n",
    "    shap_values= XGboost_ML(df=df, target_col=target_col, weight_col=weight_col, n_estimators=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
