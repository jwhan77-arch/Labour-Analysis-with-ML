{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a531c9-35bc-4fc3-a310-a358f69fec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import seaborn as sns\n",
    "\n",
    "#폴더에서 csv 전체를 읽고, 기준 데이터에서 피처를 선정\n",
    "\n",
    "def read_csv_and_select_feature(folder_path, drop_col, str_col, del_if_zero):\n",
    "    \n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    if not csv_files:\n",
    "        print (\"csv 파일을 찾을 수 없습니다.\")\n",
    "    else:\n",
    "        for file in csv_files:\n",
    "            print(f\"{os.path.basename(file)}\")\n",
    "        print(f\"총{len(csv_files)}개의 파일을 찾았습니다\\n\")\n",
    "\n",
    "    # 파일명을 통해 가장 최근 데이터를 선정하고 그 파일의 칼럼을 통합할 데이터의 기준으로\n",
    "    most_recent_file = None\n",
    "    max_year=0\n",
    "    year_pattern = re.compile(r'\\d{4}')\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        match = year_pattern.search(file_name)\n",
    "        if match:\n",
    "            current_year= int(match.group())\n",
    "            if current_year > max_year:\n",
    "                max_year = current_year\n",
    "                most_recent_file= file\n",
    "\n",
    "    try: \n",
    "        benchmark_col = pd.read_csv(most_recent_file, encoding=\"cp949\", nrows=0)\n",
    "    except UnicodeDecodeError:\n",
    "        benchmark_col = pd.read_csv(most_recent_file, encoding=\"uft-8\", nrows=0)\n",
    "    print (f\"가장 최근 조사인 {max_year}년 데이터에 {len(benchmark_col.columns)}개의 칼럼이 있습니다.\\n\")\n",
    "    \n",
    "    # drop_col에 있는 제외할 칼럼을 제거. \n",
    "    if drop_col: \n",
    "        all_col = benchmark_col.columns.tolist()\n",
    "        filtered_col = [col for col in all_col if not any(re.search(keyword, col) for keyword in drop_col)]\n",
    "        print(f\"제외할 칼럼은 {drop_col}. {len(all_col)-len(filtered_col)}개의 칼럼이 제거되었습니다.\\n{len(filtered_col)}개의 칼럼을 조사합니다.\\n\")\n",
    "    else:\n",
    "        filtered_col = benchmark_col.columns.tolist()\n",
    "    #선택된 피쳐(열)가 있는 칼럼을 파일에서 읽은 후 데이타를 병합.\n",
    "    df_list=[]\n",
    "    warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "    \n",
    "    for file in csv_files:\n",
    "        \n",
    "        try: \n",
    "            df_col=pd.read_csv(file, encoding=\"cp949\", nrows=0).columns\n",
    "            csv_encoding=\"cp949\"\n",
    "        except UnicodeDecodeError:\n",
    "            df_col=pd.read_csv(file, encoding=\"utf-8\", nrows=0).columns\n",
    "            csv_encoding=\"uft-8\"\n",
    "    \n",
    "        col_to_load= list(set(filtered_col) & set(df_col))\n",
    "        if col_to_load != filtered_col:\n",
    "            print(f\"{os.path.basename(file)}에 {list(set(filtered_col) - set(df_col))}의 데이타가 없습니다.NaN으로 채웁니다.\")\n",
    "        df= pd.read_csv(file, encoding=csv_encoding, usecols=col_to_load)\n",
    "        df=df.reindex(columns=filtered_col)\n",
    "        \n",
    "        df=df[1:]\n",
    "        df_list.append(df)\n",
    "    \n",
    "    merged_df=pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\n총 데이터는 {len(merged_df.columns)}개의 열, {len(merged_df)}개의 행입니다.\\n\")\n",
    "\n",
    "    #str_col은 문자 코드. 숫자 코드로 변환\n",
    "    print(f\"{str_col}은 숫자 코드로 변환합니다.\\n\")\n",
    "    all_col=merged_df.columns.tolist()\n",
    "    numberic_col = list(set(all_col) - set(str_col))\n",
    "    for col in numberic_col:\n",
    "        merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "    le=LabelEncoder()\n",
    "    for col in str_col:\n",
    "        merged_df[col]=le.fit_transform(merged_df[col])\n",
    "    \n",
    "     #모델에 중요한 영향을 미치기 때문에 0이 되면 안 되는 피쳐 del_if_zero에서 값이 0인 샘플을 삭제\n",
    "    print(f\"{del_if_zero}에서 0이 될 수 없는 샘플을 삭제합니다.\\n\")\n",
    "    len_before=len(merged_df)\n",
    "    for col in del_if_zero:\n",
    "        merged_df=merged_df[merged_df[col]>0]\n",
    "    len_after =len(merged_df)\n",
    "    print(f\"{len_before - len_after}개의 샘플을 제거했습니다.\\n\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "#_____________________________________________________________________\n",
    "#0을 평균으로 대체, 0이 과반 이상이면 삭제\n",
    "\n",
    "def zero_processing(df):\n",
    "        \n",
    "    df_temp=df.replace(0, np.nan)\n",
    "    df_mean=df_temp.mean().round()\n",
    "    df_semifinal=df_temp.fillna(df_mean)\n",
    "    df_final=df_semifinal.fillna(0).astype(int)\n",
    "    print(\"0을 평균값으로 대체. 열의 값이 모두 NaN인 경우 0으로 대체\")\n",
    "    \n",
    "    #0이 과반 이상인 열을 삭제\n",
    "    zeros_count = (df_final == 0).sum()\n",
    "    columns_to_drop = zeros_count[zeros_count > (len(df_final) / 2)].index.tolist()\n",
    "    print(f\"\\n0의 값이 절반을 초과하는 열: {columns_to_drop}을 삭제.\\n\")\n",
    "    df_final = df_final.drop(columns=columns_to_drop)\n",
    "    print(f\"최종 {len(df_final.columns)+1}개의 피쳐를 조사합니다.\")\n",
    "    return df_final\n",
    "\n",
    "#_____________________________________________________________________\n",
    "#총근로시간, 총임금 계산\n",
    "\n",
    "def total_hours_and_wage(df, keywords):\n",
    "\n",
    "    df['상여금성과급총액']=df['상여금성과급총액']/12 #상여금성과급총액은 1년 단위로 집계. 월단위로 재계산.\n",
    "    all_col=df.columns.tolist()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        filtered_col = [col for col in all_col if re.search(keyword, col)]\n",
    "        df[keyword]=df[filtered_col].sum(axis=1)\n",
    "        df=df.drop(columns=filtered_col)\n",
    "        print(f\"\\n<{keyword}> 칼럼 생성. {filtered_col} 칼럼 삭제.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#XGboost 머신러닝\n",
    "\n",
    "def XGboost_ML(df, target_col, weight_col, n_estimators=30):\n",
    "    \n",
    "    X = df.drop([target_col, weight_col], axis=1)\n",
    "    y= df[target_col]\n",
    "    w= df[weight_col]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(X, y, w, test_size=0.3, random_state=42)\n",
    "    \n",
    "    model= xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=n_estimators, random_state=42)\n",
    "    model.fit(X_train, y_train, sample_weight=w_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred, sample_weight=w_test))\n",
    "    mean = y_test.mean()\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"\\nXGBoost 모델 성능 (RMSE): {rmse:.2f} \\n 타깃 평균은 {mean:.2f}.\\n 평균 대비 RMSE는 {rmse/mean:.2%}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\\nXGBoost 모델의 SHAP 플롯을 생성합니다.\\n\")\n",
    "    X_train_10= X_train.sample(frac=0.01, random_state=1)\n",
    "    explainer = shap.Explainer(model, X_train_10)\n",
    "    s_values= explainer(X_train_10)\n",
    "    plt.rc('font', family='NanumGothic')\n",
    "    shap.plots.bar(s_values, max_display=10)\n",
    "   \n",
    "    print(\"--- XGBoost 모델 분석 완료 ---\")\n",
    "    return s_values\n",
    "\n",
    "#_____________________________________________________________________\n",
    "#Deep-Learning 모델\n",
    "def deep_learning(df):\n",
    "\n",
    "    print(\"ANN 모델 분석을 시작합니다.\", \"*\"*10)\n",
    "\n",
    "    #훈련용, 검증용, 시험용 데이터 나누기\n",
    "    X = df.drop([target_col, weight_col], axis=1)\n",
    "    y= df[target_col]\n",
    "    w= df[weight_col]\n",
    "    X_train, X_temp, y_train, y_temp, w_train, w_temp = train_test_split(X, y, w, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val,y_test, w_val, w_test =train_test_split(X_temp, y_temp, w_temp, test_size=0.5, random_state=36)\n",
    "\n",
    "    # 피쳐 데이타 정규화\n",
    "    mean = X_train.mean()\n",
    "    X_train -=mean\n",
    "    std= X_train.std()\n",
    "    X_train /=std\n",
    "    X_val -=mean\n",
    "    X_val /=std\n",
    "    X_test -= mean\n",
    "    X_test /=std\n",
    "\n",
    "    import keras\n",
    "    from tensorflow.keras import layers\n",
    "    model= keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(0.1), loss=\"mse\", metrics=[\"mae\"])\n",
    "    history= model.fit(X_train, y_train, epochs=5, batch_size=512, sample_weight=w_train, validation_data=(X_val, y_val,w_val))\n",
    "    \n",
    "    print(\"분석을 끝냈습니다. 결과는 Keras의 History를 참조\")\n",
    "\n",
    "    return history\n",
    "\n",
    "#_____________________________________________________________________\n",
    "#데이터 기본 분석\n",
    "\n",
    "def basic_analysis(df,target_col, weight_col):\n",
    "    \n",
    "    #Heat_Map 전처리\n",
    "    from statsmodels.stats.weightstats import DescrStatsW\n",
    "    weights =df[weight_col]\n",
    "    df_data=df.drop(columns=weight_col)\n",
    "    weighted_stats = DescrStatsW(df_data, weights=weights)\n",
    "    weighted_corr_matrix = weighted_stats.corrcoef\n",
    "    corr_df = pd.DataFrame(weighted_corr_matrix, columns=df_data.columns, index=df_data.columns)\n",
    "    corr_df_s =corr_df.stack().reset_index(name='correlation')\n",
    "    \n",
    "    \n",
    "    plt.rc('font', family='NanumGothic')\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    #히트맵\n",
    "    g=sns.relplot(data=corr_df_s, x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\", palette=\"vlag\",hue_norm=(-1, 1), edgecolor=\".7\",\n",
    "        height=10, sizes=(50, 250), size_norm=(-.2, .8),)\n",
    "    g.set_xticklabels(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    #타깃 값 히스토그램\n",
    "    threshold= df[target_col].quantile(0.99)\n",
    "    \n",
    "    sns.displot(data=df,bins=1000, x=target_col, weights=weight_col, kde=True)\n",
    "    plt.xlim(0,threshold)\n",
    "    plt.show()\n",
    "\n",
    "    #중요 변수 히스토그램\n",
    "    \n",
    "    max_list=corr_df.nlargest(5,target_col,keep='last').index\n",
    "    classification_col=[\"코드\",\"여부\"]\n",
    "    for col in max_list:\n",
    "     if col != target_col:\n",
    "         if any(keyword in col for keyword in classification_col): \n",
    "             sns.displot(data=df, x=target_col, weights=weight_col, hue=col, kind='kde')\n",
    "             plt.xlim(0,threshold)\n",
    "             plt.show()\n",
    "         else:\n",
    "             sns.displot(data=df, x=target_col, y=col)\n",
    "             plt.xlim(0,threshold)\n",
    "             plt.show()\n",
    "    \n",
    "#_____________________________________________________________________\n",
    "    \n",
    "def main():\n",
    "    \n",
    "        # 중요 파라미터들\n",
    "    folder_path=\"data2\" #csv 파일이 있는 폴더\n",
    "    keywords=[\"근로시간수\", \"액\"]\n",
    "    \n",
    "    \n",
    "    drop_col = None #변수 설명하는 칼럼들\n",
    "    str_col = ['산업대분류코드'] #분류코드가 String인 칼럼\n",
    "    del_if_zero =[\"정액급여액\"] #값이 0인 경우 삭제해야 하는 샘플\n",
    "    #enterance_date_col=\"현재일관련사항_직장시작연월\"\n",
    "    #census_date_col =\"조사연월\"\n",
    "    weight_col = \"가중값\"\n",
    "    target_col=\"액\"\n",
    "    \n",
    "    #함수 호출\n",
    "    df= read_csv_and_select_feature(folder_path=folder_path, drop_col=drop_col, str_col=str_col, del_if_zero=del_if_zero)\n",
    "    #df= zero_processing(df=df)\n",
    "   # df= len_of_serv_calculator(df=df, enterance_date_col_name=enterance_date_col, census_date_col_name=census_date_col )\n",
    "    df = total_hours_and_wage(df=df,keywords=keywords)\n",
    "    basic_analysis(df, target_col=target_col, weight_col=weight_col)\n",
    "    shap_values= XGboost_ML(df=df, target_col=target_col, weight_col=weight_col, n_estimators=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c0d557-8623-4238-91cd-e202ced33af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_총괄_20250827_24235.csv\n",
      "2022_총괄_20250827_24235.csv\n",
      "2023_총괄_20250827_62500.csv\n",
      "2024_총괄_20250827_62500.csv\n",
      "총4개의 파일을 찾았습니다\n",
      "\n",
      "가장 최근 조사인 2024년 데이터에 29개의 칼럼이 있습니다.\n",
      "\n",
      "2021_총괄_20250827_24235.csv에 []의 데이타가 없습니다.NaN으로 채웁니다.\n",
      "2022_총괄_20250827_24235.csv에 []의 데이타가 없습니다.NaN으로 채웁니다.\n",
      "2023_총괄_20250827_62500.csv에 []의 데이타가 없습니다.NaN으로 채웁니다.\n",
      "2024_총괄_20250827_62500.csv에 []의 데이타가 없습니다.NaN으로 채웁니다.\n",
      "\n",
      "총 데이터는 29개의 열, 3959117개의 행입니다.\n",
      "\n",
      "['산업대분류코드']은 숫자 코드로 변환합니다.\n",
      "\n",
      "['정액급여액']에서 0이 될 수 없는 샘플을 삭제합니다.\n",
      "\n",
      "0개의 샘플을 제거했습니다.\n",
      "\n",
      "\n",
      "<근로시간수> 칼럼 생성. ['소정실근로시간수', '초과실근로시간수', '휴일실근로시간수'] 칼럼 삭제.\n",
      "\n",
      "<액> 칼럼 생성. ['정액급여액', '초과급여액', '상여금성과급총액'] 칼럼 삭제.\n",
      "------------------------------\n",
      "\n",
      "XGBoost 모델 성능 (RMSE): 1844.59 \n",
      " 타깃 평균은 4405.24.\n",
      " 평균 대비 RMSE는 41.87%\n",
      "------------------------------\n",
      "\n",
      "XGBoost 모델의 SHAP 플롯을 생성합니다.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 27523/27714 [02:47<00:01]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XGBoost 모델 분석 완료 ---\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ea1bf-99e0-4b7a-b10c-cb1a780f9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
